# Optional: Specify which Ollama model to use (default: llama3.2)
OLLAMA_MODEL=llama3.2

# Optional: Ollama host if not running on localhost
# OLLAMA_HOST=http://localhost:11434
